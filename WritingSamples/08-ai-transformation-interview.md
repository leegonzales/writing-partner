# Interview: AI Transformation

## 1. Why AI Projects Fail

**Q1. "Everyone sees flashy AI demos, but most business efforts don't move the needle. In your view, what's the #1 reason most AI efforts in organizations fail — even with good tools?"**

Here's what should terrify every leader: MIT just published research showing 95% of organizations see zero return on their AI initiatives—not modest returns, zero. And I trust this study because they looked at the actual business outcomes six months post-pilot. Now we can argue that most pilots of new technology are troubled, but where I think this issue is problematic is what businesses are doing.

Most are pouring money into AI tools, then pushing usage, tracking utilization rates, delivering mandates like "Use AI or else"—and that "or else" is a not-so-subtle statement that you'll become obsolete and probably lose your job if you don't get with the program.

They teach people prompt engineering, they hold "prompt parties," they send people prompt guides. They mandate usage, they create change management programs helmed by L&D teams or individual functional managers. And they predictably fail or achieve only modest results.

RAND just interviewed data scientists across multiple sectors to understand why, and the five leading causes of AI project failure were all human factors—unclear business value, workforce gaps, leadership disconnects, ethical concerns, poor data practices. Not technology problems. People problems. Data problems.

There's this rule that BCG identified that I think captures it well: 10% of AI transformation is about algorithms, 20% is technology and data, and 70% is people and processes. Most organizations spend 90% of their budget on that first 30%. And even when they focus on the people side, they often miss that AI experiments need fertile ground—high-quality data, systems that AI can actually navigate, and information that isn't siloed across departments. Like usual this is all about effective change management, good data, and solving real problems as the real focus.

Beyond the basics this change is so massive, so deep, and so serious we also need to help people make sense of this moment, we need to help people move through the stages of grief as we all collectively wrestle with this civilization change.

People are facing down the prospect of massive job dislocation, the need to reinvent not just how they work, how they have made their way through the world as professionals, but also how they see themselves. Most people, when asked "what do you do," they name their profession, their education—their personality and identity are deeply anchored in the skills and work they've been doing up until now to feed their families and feel a sense of agency in their lives. And now they're faced with an almost cataclysmic shift in everything. Recent data shows 52% of employees are more concerned than excited about AI—that's up from 37% just a couple years ago. 70% fear job loss across all generations. This isn't an education gap—it's a survival response.

Now let's remember, we're just recovered from the wild ride that was COVID. We're dealing with incredible uncertainty here in the US with our absolutely insane political moment. And now we're generally asking people to figure out AI, and frankly, a lot of them are just ignoring it, hating on it, and taking a wait-and-see attitude. Meanwhile executives are losing their minds with FOMO—real or hype-driven—and demanding to see change and results from their AI investments. And it's just not happening.

What do they need? They need a humane change management program that can reliably move people from AI Passenger to AI Pilot. That's a set of terms our Labs team coined here at BetterUp. Their research findings essentially found that the most impactful mindsets that strongly indicated people would achieve results with AI were a profound sense of Agency and Optimism with AI. That is, people felt comfortable and empowered using AI technology, and they believed that they could build a future they wanted with AI as a tool or collaborator.

**Q2. "You've worked with many companies — can you share a case where leadership thought AI would solve everything, but it didn't — and why?"**

You know, I haven't worked with all that many companies on AI—most of my experience is with BetterUp, but I have worked privately with a couple of others. And what I mainly see is that leaders often under or overestimate what AI can do—and that oftentimes they don't think it can solve everything, but they really don't know what it can solve.

At BetterUp, our Data & Insights team were early AI adopters with strong results—people saving 40 plus hours a month, doing work in hours that used to take days. Then they came back with this assessment: "We've captured all the low-hanging fruit in individual workflows. Further gains require increasingly complex implementations for diminishing returns." Individual productivity had plateaued. The next phase required making our systems "AI-navigable," building semantic layers—unglamorous infrastructure work, not just more tool usage. This pattern of, we get strong initial gains and then hit a wall seems common and related to the usual patterns of data quality, and frankly the need to look more across complete workflows in organizations, not just in functional silos.

My number one advice for leaders is twofold. The first thing I tell them is: you have to put your oxygen mask on first, and you have to lead with AI. You have to learn how the tools work and actively figure out what works and doesn't work in your field. Take classes, automate some of your work, go play with Claude, ChatGPT, and Gemini. And share what you learn along the way with your team. Be public, vocal, and thoughtful about your learning. Our AI Passenger and Pilot research confirms this finding, and the broader research on change management confirms this finding.

Most leaders miss this part: share your failures, not just your polished successes. When leaders only showcase wins, they create performance pressure that kills experimentation. Psychological safety requires leaders to model vulnerability—to show that trying things, failing, and learning is not just acceptable but essential.

The second thing: stop trying to build perfect proprietary AI systems before deployment. The research is clear that organizations partnering with AI vendors and iterating together see dramatically better outcomes than those trying to build everything from scratch. The AI landscape is evolving too fast for a "perfect it first" approach to work.

**Q3. "If someone says 'but we have the best AI tools' — what would you ask them next to uncover the real gap?"**

I would ask them: what are the top use cases across your company where you're seeing real measurable outcomes from AI usage of those tools? Not activity metrics—outcome metrics. Not "how many people are using AI" but "what business results have changed because of AI?"

When you mandate AI usage with "use AI or else" pressure, people are going to do a few different things. They'll use AI performatively to check compliance boxes. Or they'll figure out how to use AI to make themselves more efficient or effective and then use that free time for something else important to them—something that may or may not align with organizational priorities. Neither of those creates transformation. I would ask them to check in with their people on how they feel, how AI is impacting real work, and I would encourage them to listen.

I would encourage them to measure learning velocity—the speed at which breakthrough AI practices propagate across their organizations. If someone discovers a game-changing AI workflow today, how long until three other teams outside their group adopt it? That acceleration in how fast organizational learning happens—not individual productivity gains—that's what creates competitive advantage.

And there's real urgency here. AI-native competitors like Cursor are hitting $100 million in annual revenue in their first year with 12 employees—a trajectory that traditionally took 8-10 years and hundreds of people. When you see that kind of compression, you realize the competitive moat has shifted from "first mover" to "fastest learner."

---

## 2. Mindset, Culture & Psychological Safety

**Q4. "A big theme you talk about is psychological safety around AI — how safe people feel to experiment, fail, critique. Why is that so crucial when introducing AI in the workplace?"**

Psychological safety is key for creating and holding space where people can process hard emotions and perform collective sense-making together. This is the work we have to do initially when taking on the change of individual mindsets so that people and teams can actually reinvent how they work—even and especially when that is deeply uncomfortable and frankly scary.

Now we know from decades of research that psychological safety is the cornerstone of trust, that a high-functioning team needs that to perform effectively. And so we know that in this moment, a company that can't muster psychological safety will flail around and not transform with AI.

Traditional workplace threats are social—looking foolish, being embarrassed. AI workplace threats are existential—professional survival. When people feel their core professional identity is under threat, it's not just uncomfortable, it's neurological. There's research on this that I find compelling—when people feel threatened, their prefrontal cortex responsible for learning and innovation shuts down. Fight-or-flight responses take over. This is biological, not just attitudinal. You can't train or mandate your way past that.

You can't transform with AI if people don't feel safe to fail. No safety, no experiments. No experiments, no learning. No learning, no competitive advantage.

**Q5. "What are signs a team doesn't have psychological safety with AI, and how would you start repairing that?"**

The signs are pretty clear once you know what to look for. People only share polished successes, never their failures or half-baked experiments. They wait for explicit permission before trying anything new with AI. You see performative usage—people using AI just enough to check compliance boxes but not actually changing how they work. Questions that should be asked in public channels get asked privately instead, or not at all. Knowledge stays siloed within individual teams rather than propagating across the organization. And you hear phrases like "I don't want to look stupid" or "I'm afraid I'll break something" or "I'm not sure I'm allowed to do that" or "I feel so behind, am I going to lose my job".

The deeper sign: utilization metrics look fine, but business outcomes aren't changing. People are using the tools, but not experimenting with them. Lots of people are using these tools to write email, or do basic research. They are barely scratching the surface of what is possible, and worst of all they are totally ignorant of this fact.

Repairing it starts with leadership speaking publicly about how they intend to move forward with AI. They need to be upfront and say: work is going to change, we don't know precisely how, and we don't know fully what it means for all of us. But we know this technology represents a state change in how we work. For now, we're going to give you the tools, we're going to encourage all of us to use this technology safely, and we will experiment together to figure out what works.

As we go along, some of you are really going to figure this out. We will reward those people who excel and figure out how to leverage this technology. Everyone else, we expect you to learn from these pioneers and make space in your days and on your projects to apply AI.

Initially, we expect AI to be most applicable in some key areas. We'll identify these early use cases and figure them out together. We'll share out what we learn and help everyone figure this out.

Beyond that honest framing from leadership, you need systematic approaches.

At BetterUp, I built AI Flight School with psychological safety, experimentation, and collective sense making as the foundation. In Session 1, we do what we call a "hot start"—we ask people to create with AI before we teach them anything. Create first, learn second. That establishes that safe failure is expected and valued, not just tolerated. We teach them to embrace their Agency, and to stop being afraid of the technology, and the future. Instead we work to encourage them to see AI as a machine for achieving their goals. As a collaborator for sharpening their thinking, and as an ally they can reliably hand their drudgery too.

We explicitly name the range of emotions people feel. We have "Show and Tell" segments where people share failures openly, not just successes. We elicit peoples fears and dark thoughts with AI. I share my own AI mistakes first. I share my fears as well.

Psychological safety doesn't mean everyone feels comfortable all the time. It means people feel safe being uncomfortable together, safe learning in public, safe admitting what they don't know, and safe saying they are afraid. Then we work through that all together.

**Q6. "You often emphasize mindset over tools — what are the core mindset shifts teams need to make to succeed with AI?"**

OK, there are several. I'll rattle off a few. A learner's mindset is key—a strong belief in personal agency and an optimistic view that you can influence your future with your efforts. That you can learn, keep up with, and leverage AI to build the kind of life you want. I also think it's worth underlining that grit and cognitive agility are pretty fundamental.

I like to tell the story of a master carpenter whose hammer changes in their hands every couple of weeks. That is what it is like with AI right now. Although there are many things that stay fairly evergreen, like prompt engineering and context engineering—getting good at managing your information diet and staying up to date on the latest model releases and features per tool becomes essential. Ethan Mollick describes this notion that the AI models are pushing out this jagged frontier, and so it's necessary for people to keep updating their skills and approaches as the frontier moves. Honestly, that's best done in groups, and so I see a community here as key too.

The other big mindset shift is from Passenger to Pilot. Passengers use AI as a black box, accept outputs without iteration, work individually. Pilots actively shape AI to their specific needs, iterate systematically, document and propagate learnings. The difference isn't technical skill—it's about agency and whether you believe you can steer.

And then there's the organizational mindset shift around how you measure success. Measuring the diffusion rate of learning across an organization is really key. How fast do breakthrough practices spread from pioneers to three other teams? That propagation speed—learning velocity—is the metric that determines competitive advantage. Is that a mindset? It certainly is.

---

## 3. AI Literacy & Fluency

**Q7. "When people say 'we need AI training' — what do they usually get wrong? What does real AI literacy look like?"**

Real AI literacy looks like a deep commitment to continuous learning and rigorous testing. It means keeping your head on straight as you work with these tools and building up a sense of what is true and what is BS—both from the models and from the hype agents in the ecosystem.

It means knowing enough about how the underlying technology works to be able to reason about it logically. Like knowing how many tokens fit into Claude or ChatGPTs context window, or what makes for a good prompt or how RAG works conceptually. It means building up enough reps with the technology to intuitively have a sense of what will or won't work with the AI technology of today, and what to look for in the next model updates to know you have to update your intuitions. And ideally it means having the tests written down from your actual work and life so you can quickly assess the new models when they come out.

I also think a lot of people think AI training equals transformation. Most "AI training" programs teach prompt engineering formulas, measure course completions and tool utilization rates, and then wonder why business outcomes don't change.

Real literacy or I would better say proficiency is demonstrated through impact. In our certification programs, people don't pass an exam—they build a portfolio. They show us that they can create workflow transformation via case studies, they show us AI toolkits with prompts they actually use, Claude Projects they use daily, or CustomGPTs others are regularly using. They show how they have helped their peers, and team.

**Q8. "How do you design programs where people don't just passively learn about AI, but internalize it — think like AI, challenge it, work with it?"**

You have to change mindsets, build skill sets, and then burn in habits. I tell people routinely that they have to build their AI flywheels—systems that compound in value over time—and that whenever they start any project, to start with AI as a thought partner. Then it helps to have accountability with other people and make a learning plan with goals and actions.

We start with creation and experimentation, not instruction. We throw people in and help them swim in the deep water of learning and uncertainty. Its not comfortable, thats OK and on purpose.

In all of our sessions of AI Flight School, we do a "hot start"—we give people a real work problem or an exceptional prompt to play with or use as an AI coach. It is a lot of "show" not "tell". We also let people fail safely. We have them debrief with each other in small groups: What worked? What felt uncomfortable? What questions emerged?

We create space for experimentation, and uncomfortable conversations. We allow individual fears to surface in a supportive context, and demonstrate that AI is a tool for exploration, not a system to master perfectly before using. We help people move from a fear based, I am running away from something, AI change, to running towards something… I can be empowered with AI, and have it help me build the life I want.

And in the end, really, we have to get people intrinsically motivated to learn and change and grow. Once we get them excited and engaged with AI—especially when they see it solve real problems for them and save them real time—they tend to get excited and really change.

**Q9. "What's one concrete exercise or practice that helps teams build AI fluency even if they're nontechnical?"**

Oh, that's easy—we do that in session 2 or 3 of AI Flight School. We tell the participants to have ChatGPT or Claude build them a customized prompt engineering learning program. Then we have them share that with the other participants. It's such an obvious thing, but you can use the tools to learn how to use the tools.

We also teach them meta-prompting pretty early, which means they learn how to have the AI build them high-quality prompts. So instead of memorizing formulas, they learn how to have AI craft prompts for their specific needs.

We also make them watch videos from the different model makers' academies, and read blog posts from the best thinkers. The real practice that jump starts fluency are the hot starts I mentioned—creating first without instruction or experimenting with incredibly good prompts. So many often exclaim, I had no idea you could do this with AI. I just smile and say, folk the best is yet to come.

---

## 4. Obstacles, Myths & Underexplored Questions

**Q10. "What's a myth you hear often about business AI projects that you wish leaders would stop believing?"**

You know I am not sure, but what I would tell them is that the AI tools and models are frankly the easy part. The hard part is the high-quality data, the high-quality evals, the systems designed so AI can navigate them, the reinvention of fundamentals workflows, and of course motivating people to grow, learn and change.

I think some leaders believe that "if we just get the best AI tools and hire smart people, transformation will happen." Change has to come from the grassroots and the very top of the organization. It has to be thoughtful, incremental, and comprehensive.

**Q11. "In your work, what's one issue or question that tends to get overlooked — something you wish people asked more often?"**

Just because you can use AI to automate something, should you?

I think of the value of a given work stream along three dimensions of value creation. There's the value you get from doing the task—let's say you're building a report for your head of sales on your competitors. You can clearly have AI help you do that research and make positioning recommendations. But the question is how.

Beyond producing a good report, there are two more kinds of value that I see. There's what you're learning in doing the work, right? That's building your own skills, knowledge, and ability. Then third, you're likely building rapport and relationships when working with others on the thing you are doing.

Thoughtfully bringing AI into that flow of work is key. Otherwise you risk losing much of the value of the task beyond the concrete deliverable. Maybe AI does the initial research scan, but you do the synthesis. Maybe AI drafts sections, but you have conversations with stakeholders about what resonates.

The other overlooked question: have you created fertile ground for AI to actually work? People try AI, get garbage results from fragmented data or siloed systems, and conclude "AI doesn't work here" when really, they didn't prepare the infrastructure. Or they don't really know how to use it, and conclude that if it does not work the first time it must be crap.

**Q12. "If you had just 5 minutes with a CEO pushing AI adoption too fast, what would you tell them to pause, reflect on, or do differently?"**

I would ask them: what is it you want to achieve with AI adoption? What are your goals?

Most will say something like "competitive advantage" or "efficiency gains," and I'd dig deeper. Competitive advantage how? Efficiency toward what end?

I would want them to ask themselves the question: what sort of change are we facing right now, and what level of investment, effort, and care is truly required? And then act accordingly.

People are exhausted from constant change. If you push adoption too fast without addressing the human dynamics, you'll get performative compliance—high utilization metrics with low impact—or you'll get backlash and resistance.

Neither gets you competitive advantage.

What I'd tell them to do: First, put your own oxygen mask on. Spend real time learning these tools yourself. Share your learning—including failures—publicly with your team. Second, create psychological safety before demanding productivity. Third, measure learning velocity, not just utilization. How fast do breakthrough practices spread in your organization? That determines competitive advantage.

And yes, there's real urgency—maybe a 12-18 month window before the gap between fastest learners and everyone else becomes very hard to close. Urgency applied to the wrong things—mandates, compliance, tool rollouts—wastes that window. Urgency applied to the right things—psychological safety, learning velocity, systematic knowledge propagation—compounds.

---

## 5. Personal & Aspirational

**Q13. "What parts of working on AI transformation get you excited — what do you love talking about with peers?"**

Oh man, mostly all of it. But I really like the concrete problem-solving most of all. I spend a lot of time weekly with my students and users talking about how they're applying or trying to apply AI. And I learn a lot from people who are running experiments and trying things. Then I work to package that all up and share it with others.

I love when someone posts in our Slack channel, "I tried this thing and it failed spectacularly, but here's what I learned." That's gold. Because learning is impact. That failure just accelerated everyone else's understanding.

I love watching the mindset shifts. Someone goes from "I'm completely intimidated to post among the 'AI gods'" to contributing their own breakthroughs. That transformation from Passenger to Pilot—that's what this is all about.

And honestly, I love the urgency of this moment. We're watching AI-native companies hit $100 million in revenue with tiny teams in months, while traditional companies take years with hundreds of people. The competitive landscape is shifting in real-time. That urgency focuses the mind wonderfully.

**Q15. "If you were to pick three things you'd want every leader to really get about AI adoption — what are those three?"**

First: This is about identity and culture. You have to really internalize the point of view of your staff to really make this work.

For many people, their profession is their identity—software engineering isn't just what they do, it's woven into who they are. AI threatens that at an existential level. If you don't address those identity concerns, create psychological safety, and help people see how they can build the future they want with AI, they won't transform. They'll comply performatively or resist actively. Neither creates competitive advantage.

Second: We are all building the future we want to live in right now. Do we want to have AI create the sort of jobs, companies, and society we would want for our children? Or is it all about profit, margin, and shareholder value?

I can tell you I really fear we're going to slowly all move into a circular firing squad. As AI gets better and better, we will start to see real job loss due to it, and we have to decide if that is the world we want or if there is another path. The decisions leaders make today—about how to deploy AI, how to treat people in this transition, how to measure success—those decisions are building the future. This isn't about replacing humans—it's about augmenting human capability in ways that respect both human agency and human potential.

Lastly: You are probably wildly overestimating what AI can do in the short term and wildly underestimating what AI can do in the mid to long term. We're looking at nothing short of a complete reinvention of how we work. And the models are getting exponentially more capable.

I would suggest leaders spend some time with the AI systems imagining the future of their work and their companies. Write some science fiction. Imagine what happens when AI systems can work autonomously for days on end and do real groundbreaking research. That future is coming faster than most people think.

**Q16. "In your ideal future, after organizations really get this right, what does that world look like in how people work, learn, and lead with AI?"**

I think that world can be really amazing, where we seamlessly collaborate with and create our AI agents. These swarms of AI systems can free us from mass drudgery and bullshit jobs.

I envision a world where people are doing more of the work that feeds the human spirit—more creative work, more work in partnership with other humans, more deep thinking, more strategic work, more relational work, more building, more designing.

I also think companies and organizations get smaller and much, much more effective and powerful. We're already seeing AI-native companies achieve millions in revenue per employee versus a few hundred thousand for traditional companies. That's not just efficiency—that's fundamentally different economics.

I want this transformation to be human-centered. I want organizations to measure learning velocity, not just utilization. I want psychological safety to be foundational. I want people to develop AI fluency that empowers them rather than threatens them. I want breakthrough practices to propagate in weeks, not months.

I don't really know where this all ends, but what I do know is that I want many, many more people's voices in the mix as we make this future. Not just tech people, not just executives, but everyone. The more diverse perspectives we have shaping AI transformation, the better chance we have of building something truly worth building.

That's the world I'm working toward.
